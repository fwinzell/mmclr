import torch
# Loss functions from PatchGCN

def nll_loss(hazards, S, Y, c, alpha=0.4, eps=1e-7):
    batch_size = len(Y)
    Y = Y.view(batch_size, 1) # ground truth bin, 1,2,...,k
    c = c.view(batch_size, 1).float() #censorship status, 0 or 1
    if S is None:
        S = torch.cumprod(1 - hazards, dim=-1) # surival is cumulative product of 1 - hazards
    # without padding, S(0) = S[0], h(0) = h[0]
    S_padded = torch.cat([torch.ones_like(c), S], 1) #S(-1) = 0, all patients are alive from (-inf, 0) by definition
    # after padding, S(0) = S[1], S(1) = S[2], etc, h(0) = h[0]
    #h[y] = h(1)
    #S[1] = S(1)
    uncensored_loss = -(1 - c) * (torch.log(torch.gather(S_padded, 1, Y).clamp(min=eps)) + torch.log(torch.gather(hazards, 1, Y).clamp(min=eps)))
    censored_loss = - c * torch.log(torch.gather(S_padded, 1, Y+1).clamp(min=eps))
    neg_l = censored_loss + uncensored_loss
    loss = (1-alpha) * neg_l + alpha * uncensored_loss
    loss = loss.mean()
    return loss

def ce_loss(hazards, S, Y, c, alpha=0.4, eps=1e-7):
    batch_size = len(Y)
    Y = Y.view(batch_size, 1) # ground truth bin, 1,2,...,k
    c = c.view(batch_size, 1).float() #censorship status, 0 or 1
    if S is None:
        S = torch.cumprod(1 - hazards, dim=1) # surival is cumulative product of 1 - hazards
    # without padding, S(0) = S[0], h(0) = h[0]
    # after padding, S(0) = S[1], S(1) = S[2], etc, h(0) = h[0]
    #h[y] = h(1)
    #S[1] = S(1)
    S_padded = torch.cat([torch.ones_like(c), S], 1)
    reg = -(1 - c) * (torch.log(torch.gather(S_padded, 1, Y)+eps) + torch.log(torch.gather(hazards, 1, Y).clamp(min=eps)))
    ce_l = - c * torch.log(torch.gather(S, 1, Y).clamp(min=eps)) - (1 - c) * torch.log(1 - torch.gather(S, 1, Y).clamp(min=eps))
    loss = (1-alpha) * ce_l + alpha * reg
    loss = loss.mean()
    return loss

class CrossEntropySurvLoss(object):
    def __init__(self, alpha=0.15):
        self.alpha = alpha

    def __call__(self, hazards, S, Y, c, alpha=None): 
        if alpha is None:
            return ce_loss(hazards, S, Y, c, alpha=self.alpha)
        else:
            return ce_loss(hazards, S, Y, c, alpha=alpha)

# loss_fn(hazards=hazards, S=S, Y=Y_hat, c=c, alpha=0)
class NLLSurvLoss(object):
    def __init__(self, alpha=0.15):
        self.alpha = alpha

    def __call__(self, hazards, label_dict, alpha=None):
        S = None
        Y = label_dict['label']
        c = 1-label_dict['indicator']
        if alpha is None:
            return nll_loss(hazards, S, Y, c, alpha=self.alpha)
        else:
            return nll_loss(hazards, S, Y, c, alpha=alpha)
    # h_padded = torch.cat([torch.zeros_like(c), hazards], 1)
    #reg = - (1 - c) * (torch.log(torch.gather(hazards, 1, Y)) + torch.gather(torch.cumsum(torch.log(1-h_padded), dim=1), 1, Y))


class CoxSurvLoss(object):
    def __call__(hazards, S, c, **kwargs):
        # This calculation credit to Travers Ching https://github.com/traversc/cox-nnet
        # Cox-nnet: An artificial neural network method for prognosis prediction of high-throughput omics data
        current_batch_len = len(S)
        R_mat = np.zeros([current_batch_len, current_batch_len], dtype=int)
        for i in range(current_batch_len):
            for j in range(current_batch_len):
                R_mat[i,j] = S[j] >= S[i]

        R_mat = torch.FloatTensor(R_mat).to(device)
        theta = hazards.reshape(-1)
        exp_theta = torch.exp(theta)
        loss_cox = -torch.mean((theta - torch.log(torch.sum(exp_theta*R_mat, dim=1))) * (1-c))
        return loss_cox
    
# Modified Loss from SimMLM
# Survival task loss (NLLSurvLoss) + our MoFe loss
class BlendedLoss(object):
    def __init__(
            self, alpha=0.15, compute_ranking_loss=False,
            surv_loss_weight=1, ranking_loss_weight=0
    ):
        super().__init__()
        self.nll = NLLSurvLoss(alpha=alpha)
        self.compute_ranking_loss = compute_ranking_loss
        self.surv_loss_weight = surv_loss_weight
        self.ranking_loss_weight = ranking_loss_weight

    def forward(self, hazards: torch.Tensor, labels: torch.Tensor):
        """Compute Dice Loss & Binary Cross Entropy

        Args:
            hazards (Tensor): [B, C, ...]
            labels (Tensor): [B, C, ...]
        """
        
        nll_loss = self.nll(hazards, labels)

        if self.compute_ranking_loss:
            nll_loss = nll_loss.mean(dim=[_ for _ in range(2, len(nll_loss.shape))])

            nll_loss_more = nll_loss[:nll_loss.shape[0] // 2]
            nll_loss_less = nll_loss[nll_loss.shape[0] // 2:]

            nll_diff = nll_loss_more - nll_loss_less
            nll_diff = torch.clamp(nll_diff, min=0)
            ranking_loss = nll_diff.mean() 

            surv_loss = nll_loss.mean() 

            loss_dict = {
                'task_loss': surv_loss * self.surv_loss_weight + ranking_loss * self.ranking_loss_weight,
                'surv_loss': surv_loss,
                'ranking_loss': ranking_loss
            }
        else:
            loss_dict = {
                'task_loss': nll_loss.mean() 
            }

        return loss_dict